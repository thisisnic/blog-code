---
title: How can Apache Arrow be used for data engineering in R?
author: ''
date: '2021-07-06'
slug: how-can-apache-arrow-be-used-for-data-engineering-in-r
categories:
  - R
tags:
  - arrow
---

I've been working ar Ursa Computing on Apache Arrow for 3 months now, and one of the things that occurred to me is that whilst I have developed an understanding of the package functionality, I have a less in-depth understanding of *why* people might use Apache Arrow.

I was discussing this with a colleague who said that it's helpful to think of Arrow in terms of two different main use cases.

Firstly, you have the user who is wanting to work with some data that's larger than memory, and won't be working much with the functions that expose the lower-level API.  They'll most likely be using a lot of the dplyr-style non-standard-evaluation functions to conduct some sort of analysis.

On the other hand, you have users who are going to be working with the functions that allow you to manually create Arrow objects via R bindings to the C++ code.  The kind of work being performed by these users has been repeatedly referred to as "data engineering type work" and I'm going to explore that in more detail here.

I think it's probably important to start off by defining exactly what Arrow "is" are there have been [misunderstandings](https://github.com/h2oai/db-benchmark/issues/229) of this in the past.

The description on the Apache Arrow Github repo reads:

> Apache Arrow is a development platform for in-memory analytics. It contains a set of technologies that enable big data systems to process and move data fast.

In other words, Apache Arrow is a project which encompasses multiple components.  The key ones which are relevant here are:

* [a columnar in-memory data format](https://github.com/apache/arrow/blob/master/docs/source/format/Columnar.rst)
* [a streaming format so that data can be shared by different processes or environments](https://github.com/apache/arrow/blob/master/docs/source/format/Columnar.rst#serialization-and-interprocess-communication-ipc)
* the C++ reference libraries and other implementations or bindings in a large number of languages (including R)

Thus, the Arrow R package is just one of the libraries within the overall project.

# What is data engineering?

As Ian Cook highlights in [this talk](https://ianmcook.github.io/indy-user-may-2021)([see the video here](https://www.youtube.com/watch?v=SXbq4OYtsFA)), data engineers "typically build, manage, and optimize systems for transforming data into forms that facilitate analysis."

Some of the problems that a data engineer may need to solve include (again, this content is heavily paraphrased from Ian's fantastic talk, so please do go check that out!):

* how long does the data need to be stored for?
* does the data need to be stored in a human-readable format or is a binary format OK?
* what performance is needed?
* what is the cost of storing it?
* what compression algorithms do we need to use?
* which file format should we use?
* what data types should each column have?
* might the data types of the columns need to change or might we add new columns?
* where is the data stored and how will it be accessed?
* does the data need to be uploaded/downloaded?

# How does Arrow help solve these problems?

* allows users to read/write from/to Parquet files - this is binary columnar compressed format (*** TALK ABOUT SIZE AND COMPRESSION BENEFITS HERE ***)
  * allows users to select compression from a range of compression formats (** Can we compare CSV/RDS/Parquet **)
* allows users to read/write from/to Feather files - this is a representation of the file on disk so no serialisation and deserialisation is needed; not compressed as well as Parquet files, but can be super fast
* allows human-readable formats such as JSON/CSV etc
* allows for the use of fixed-precision decimal numbers to avoid floating point errors



